# -*- coding: utf-8 -*-
"""Streamlit 3.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dbWebhhWNFG-QiAS4lfyLTTYVeOCSig1
"""

# --- Full ML Training Script (Save this as e.g., 'train_models.py') ---

# Core libraries for data handling and numerical operations
import pandas as pd
import numpy as np

# Preprocessing tools from scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Machine learning regression models
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor



# For saving and loading models
import joblib

# Step 1: Load and Initial Data Inspection
df = pd.read_csv('market_pipe_thickness_loss_dataset.csv')
print("Original Dataset Head:")
print(df.head())
print("\nDataset Info:")
df.info()
print("\nUnique Materials:", df['Material'].unique())
print("Unique Grades:", df['Grade'].unique())
print("Unique Conditions:", df['Condition'].unique()) # For LabelEncoder

# Step 2: Define Features and Target for Regression
target_regression_feature = 'Thickness_Loss_mm'
features_for_regression = [col for col in df.columns if col not in [
    target_regression_feature, 'Material_Loss_Percent', 'Condition'
]]
X_reg = df[features_for_regression].copy()
y_reg = df[target_regression_feature].copy()

numerical_features_reg = [col for col in X_reg.columns if X_reg[col].dtype != 'object']
categorical_features_reg = [col for col in X_reg.columns if X_reg[col].dtype == 'object']

print(f"\nFeatures for Regression (X_reg): {X_reg.columns.tolist()}")
print(f"Target for Regression (y_reg): {target_regression_feature}")

# Step 3: Preprocessing for Regression Model
X_reg_encoded = pd.get_dummies(X_reg, columns=categorical_features_reg, drop_first=True)
scaler_reg = StandardScaler()
numerical_cols_after_encoding_reg = [col for col in X_reg_encoded.columns if col in numerical_features_reg]
X_reg_encoded[numerical_cols_after_encoding_reg] = scaler_reg.fit_transform(X_reg_encoded[numerical_cols_after_encoding_reg])
print("\nFeatures (X_reg) after Scaling and One-Hot Encoding Head:")
print(X_reg_encoded.head())

# Step 4: Train-Test Split for Regression
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg_encoded, y_reg, test_size=0.3, random_state=42
)

# Step 5: Define Evaluation Function for Regression
def evaluate_regression_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"\n--- Evaluating {model_name} (Regression) ---")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R-squared (R2): {r2:.4f}")
    plt.figure(figsize=(10, 7))
    sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.3}, line_kws={'color':'red'})
    plt.xlabel(f'Actual {target_regression_feature}')
    plt.ylabel(f'Predicted {target_regression_feature}')
    plt.title(f'Actual vs. Predicted {target_regression_feature} - {model_name}')
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    return mae, rmse, r2

# Step 6: Train and Evaluate Different Regression Models
regression_model_performance = {}
lr_reg_model = LinearRegression()
lr_reg_model.fit(X_train_reg, y_train_reg)
lr_mae, lr_rmse, lr_r2 = evaluate_regression_model(lr_reg_model, X_test_reg, y_test_reg, "Linear Regression")
regression_model_performance["Linear Regression"] = {"MAE": lr_mae, "RMSE": lr_rmse, "R2": lr_r2}

dt_reg_model = DecisionTreeRegressor(random_state=42)
dt_reg_model.fit(X_train_reg, y_train_reg)
dt_mae, dt_rmse, dt_r2 = evaluate_regression_model(dt_reg_model, X_test_reg, y_test_reg, "Decision Tree Regressor")
regression_model_performance["Decision Tree Regressor"] = {"MAE": dt_mae, "RMSE": dt_rmse, "R2": dt_r2}

rf_reg_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg_model.fit(X_train_reg, y_train_reg)
rf_mae, rf_rmse, rf_r2 = evaluate_regression_model(rf_reg_model, X_test_reg, y_test_reg, "Random Forest Regressor")
regression_model_performance["Random Forest Regressor"] = {"MAE": rf_mae, "RMSE": rf_rmse, "R2": rf_r2}

# Step 7: Compare Models and Choose the Best Regression Model
regression_performance_df = pd.DataFrame.from_dict(regression_model_performance, orient='index')
print("\n--- Regression Model Performance Summary ---")
print(regression_performance_df)
best_reg_model_name = regression_performance_df['R2'].idxmax()
print(f"\nBest regression model based on R2 score: {best_reg_model_name} (R2: {regression_performance_df.loc[best_reg_model_name]['R2']:.4f})")
if best_reg_model_name == "Linear Regression": best_reg_model = lr_reg_model
elif best_reg_model_name == "Decision Tree Regressor": best_reg_model = dt_reg_model
elif best_reg_model_name == "Random Forest Regressor": best_reg_model = rf_reg_model
else: best_reg_model = None
print(f"Selected best regression model object: {best_reg_model}")

# Step 8: Determine Condition Thresholds
print("\nThickness Loss distribution by Condition in original dataset:")
print(df.groupby('Condition')['Thickness_Loss_mm'].describe())

# Step 9: Save Regression Model and Preprocessing Tools
reg_model_filename = 'thickness_loss_regressor.joblib'
joblib.dump(best_reg_model, reg_model_filename)
print(f"\nBest regression model ({best_reg_model_name}) saved as '{reg_model_filename}'")
reg_scaler_filename = 'scaler_reg.joblib'
joblib.dump(scaler_reg, reg_scaler_filename)
print(f"Regression Scaler saved as '{reg_scaler_filename}'")
reg_model_features_order_filename = 'reg_model_features_order.joblib'
joblib.dump(X_reg_encoded.columns.tolist(), reg_model_features_order_filename)
print(f"Regression model feature order saved as '{reg_model_features_order_filename}'")

# --- Explicitly save LabelEncoder for Condition categories ---
le_condition = LabelEncoder()
le_condition.fit(df['Condition']) # Fit on the original Condition column
joblib.dump(le_condition, 'label_encoder.joblib')
print(f"LabelEncoder for Condition saved as 'label_encoder.joblib'")

# --- Save lists of all categories for Streamlit app's one-hot encoding consistency ---
all_materials_in_dataset_saved = sorted(df['Material'].unique().tolist())
all_grades_in_dataset_saved = sorted(df['Grade'].unique().tolist())
joblib.dump(all_materials_in_dataset_saved, 'all_materials_in_dataset.joblib')
joblib.dump(all_grades_in_dataset_saved, 'all_grades_in_dataset.joblib')
print("All material and grade categories lists saved for Streamlit app.")

print("\nAll machine learning setup and saving complete.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# # app.py
# 
# # Import necessary libraries for Streamlit, data handling, and model loading
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# 
# # --- 0. Load the original dataset within the app for dynamic options and ALL categories ---
# try:
#     original_df = pd.read_csv('market_pipe_thickness_loss_dataset.csv')
#     # Extract all unique categories for Material and Grade from the original dataset
#     # These will be used by pd.get_dummies to ensure consistent column creation.
#     all_materials_in_dataset = sorted(original_df['Material'].unique().tolist())
#     all_grades_in_dataset = sorted(original_df['Grade'].unique().tolist()) # Sort for consistent order if needed
# except FileNotFoundError:
#     st.error("Error: 'market_pipe_thickness_loss_dataset.csv' not found. Please ensure it's in the same directory.")
#     st.stop()
# 
# # --- For debugging: Display training data ranges in the sidebar ---
# with st.sidebar.expander("Training Data Ranges (for comparison)"):
#     st.write("Min values from training data:\n", original_df.min(numeric_only=True))
#     st.write("Max values from training data:\n", original_df.max(numeric_only=True))
#     st.write("Material categories:\n", original_df['Material'].unique())
#     st.write("Grade categories:\n", original_df['Grade'].unique())
# 
# 
# # --- 1. Load the Saved Machine Learning Components ---
# try:
#     # Regression model for Thickness Loss prediction
#     reg_model = joblib.load('thickness_loss_regressor.joblib')
#     # Scaler for the regression model's inputs
#     scaler_reg = joblib.load('scaler_reg.joblib')
#     # Feature order for the regression model's inputs (from ML training phase)
#     reg_model_features_order = joblib.load('reg_model_features_order.joblib')
# 
#     # Label encoder (loaded as requested, though condition is derived by thresholds now)
#     label_encoder = joblib.load('label_encoder.joblib') # This file should now be present from the updated ML training script.
# 
#     st.success("All Machine Learning components loaded successfully!")
# except FileNotFoundError as e:
#     st.error(f"Error loading ML files: {e}. Please ensure all .joblib files and .csv are in the same directory.")
#     st.stop() # Stop the app if files are not found
# 
# # --- 2. Define Features and Thresholds ---
# # Features that go into the REGRESSION model
# # This list must match the features used to train the regression model (excluding the target itself).
# numerical_features_for_model = ['Pipe_Size_mm', 'Thickness_mm', 'Max_Pressure_psi', 'Temperature_C',
#                                 'Corrosion_Impact_Percent', 'Time_Years']
# 
# # Categorical features that the REGRESSION model expects (original names)
# categorical_features_for_regression_model = ['Material', 'Grade']
# 
# 
# # Default value for Corrosion_Impact_Percent (since it's not a user input)
# DEFAULT_CORROSION_IMPACT_PERCENT = 15.0 # This value is fed into the regression model.
# 
# # Define Condition thresholds based on Thickness_Loss_mm
# # These are the thresholds you determined from your ML analysis (e.g., Step 9 in ML code)
# CONDITION_THRESHOLD_MODERATE = 2.0 # Thickness loss >= 2.0 mm means at least Moderate
# CONDITION_THRESHOLD_CRITICAL = 5.0 # Thickness loss >= 5.0 mm means Critical
# 
# # Function to map predicted thickness loss to a condition (as defined in ML Phase, Step 9)
# def get_condition_from_thickness_loss(predicted_loss_mm):
#     if predicted_loss_mm >= CONDITION_THRESHOLD_CRITICAL:
#         return 'Critical'
#     elif predicted_loss_mm >= CONDITION_THRESHOLD_MODERATE:
#         return 'Moderate'
#     else:
#         return 'Normal'
# 
# # --- 3. Streamlit App Layout and Styling (Wow Factor!) ---
# 
# # Set wide mode for the page
# st.set_page_config(layout="wide", page_title="Pipeline Integrity Predictor", page_icon="‚öôÔ∏è")
# 
# # Custom CSS for a more appealing and interactive look
# st.markdown(
#     """
#     <style>
#     .main {
#         background-color: #f0f2f6;
#         padding: 20px;
#     }
#     .st-emotion-cache-vk33v5 { /* Target the header/title element to center it */
#         text-align: center;
#         color: #2c3e50;
#     }
#     .st-emotion-cache-vk33v5 h1 {
#         font-size: 3.5em;
#         margin-bottom: 0.5em;
#         text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
#     }
#     .st-emotion-cache-vk33v5 h2 {
#         color: #34495e;
#         font-size: 1.8em;
#         margin-bottom: 1em;
#     }
#     .stButton>button {
#         background-color: #3498db;
#         color: white;
#         padding: 10px 20px;
#         border-radius: 8px;
#         border: none;
#         font-size: 1.1em;
#         transition: all 0.3s ease;
#         box-shadow: 2px 2px 5px rgba(0,0,0,0.2);
#     }
#     .stButton>button:hover {
#         background-color: #2980b9;
#         transform: translateY(-2px);
#         box-shadow: 4px 4px 8px rgba(0,0,0,0.3);
#     }
#     .prediction-box {
#         background-color: #ffffff;
#         border-left: 8px solid #3498db;
#         padding: 20px;
#         border-radius: 10px;
#         box-shadow: 0px 4px 8px rgba(0,0,0,0.1);
#         margin-top: 20px;
#     }
#     .prediction-box h3 {
#         color: #2c3e50;
#         margin-bottom: 15px;
#         font-size: 1.6em;
#     }
#     .prediction-box p {
#         font-size: 1.2em;
#         margin-bottom: 8px;
#         color: #34495e;
#     }
#     .prediction-box .highlight {
#         font-weight: bold;
#         color: #e74c3c; /* Critical */
#     }
#     .prediction-box .moderate {
#         font-weight: bold;
#         color: #f39c12; /* Moderate */
#     }
#     .prediction-box .normal {
#         font-weight: bold;
#         color: #27ae60; /* Normal */
#     }
#     .sidebar .sidebar-content {
#         background-color: #ecf0f1;
#         padding: 20px;
#         border-radius: 10px;
#         box-shadow: 0px 4px 8px rgba(0,0,0,0.1);
#     }
#     .stSlider > div > div > div { /* Make sliders more prominent */
#         background-color: #3498db;
#     }
#     .stSlider [data-baseweb="slider"] {
#         background-color: #ecf0f1;
#     }
#     .stSelectbox {
#         background-color: #ecf0f1;
#         border-radius: 5px;
#     }
#     </style>
#     """,
#     unsafe_allow_html=True
# )
# 
# st.title("‚öôÔ∏è Pipeline Integrity Prediction App")
# st.markdown("## Predict Pipeline Condition & Degradation Over Time")
# 
# st.sidebar.header("Pipeline Parameters")
# st.sidebar.markdown("Adjust the values below to simulate different pipeline scenarios.")
# 
# 
# # --- 4. User Input Collection ---
# # Arranged one by one vertically in the sidebar
# 
# pipe_size = st.sidebar.slider("Pipe Size (mm)", min_value=100, max_value=2000, value=800, step=50)
# initial_thickness = st.sidebar.slider("Initial Thickness (mm)", min_value=5.0, max_value=50.0, value=15.0, step=0.5)
# 
# material_options = original_df['Material'].unique().tolist()
# material = st.sidebar.selectbox("Material", material_options)
# 
# years_to_predict = st.sidebar.slider("Years to predict (Time in Service)", min_value=0, max_value=30, value=10, step=1)
# 
# max_pressure = st.sidebar.slider("Max Pressure (psi)", min_value=100, max_value=3000, value=1000, step=50)
# temperature = st.sidebar.slider("Temperature (¬∞C)", min_value=-10.0, max_value=100.0, value=25.0, step=0.5)
# 
# 
# # --- 5. Prediction Logic ---
# 
# # Create a button to trigger prediction
# if st.sidebar.button("Predict Pipeline Integrity"):
#     st.markdown("---") # Visual separator
# 
#     # --- 5.1 Prepare Input for Regression Model (Highly Robust Version) ---
# 
#     # 1. Start with numerical features and defaults (explicitly cast to float)
#     input_features_for_reg = {
#         'Pipe_Size_mm': float(pipe_size),
#         'Thickness_mm': float(initial_thickness),
#         'Max_Pressure_psi': float(max_pressure),
#         'Temperature_C': float(temperature),
#         'Corrosion_Impact_Percent': float(DEFAULT_CORROSION_IMPACT_PERCENT),
#         'Time_Years': float(years_to_predict),
#     }
# 
#     # 2. Add one-hot encoded categorical features manually
#     # Initialize all potential dummy columns to 0.0 (float)
#     temp_dummy_cols_dict = {}
#     # Iterate through all possible categories (from loaded lists) to ensure consistency
#     for mat_cat in all_materials_in_dataset:
#         if mat_cat != 'Carbon Steel': # Assuming 'Carbon Steel' was dropped first for Material
#             temp_dummy_cols_dict[f'Material_{mat_cat}'] = 0.0
#     for grade_cat in all_grades_in_dataset:
#         if grade_cat != 'ASTM A333 Grade 6': # Assuming 'ASTM A333 Grade 6' was dropped first for Grade
#             temp_dummy_cols_dict[f'Grade_{grade_cat}'] = 0.0
# 
#     # Set the selected Material's dummy to 1.0 (float)
#     if material != 'Carbon Steel':
#         selected_material_dummy = f'Material_{material}'
#         if selected_material_dummy in temp_dummy_cols_dict:
#             temp_dummy_cols_dict[selected_material_dummy] = 1.0
# 
#     # Combine numerical and manually created dummy features
#     all_input_features_combined = {**input_features_for_reg, **temp_dummy_cols_dict}
# 
#     # Create the final DataFrame for the model, ensuring exact order and all columns are present
#     # This also handles columns not present in all_input_features_combined (sets them to 0.0)
#     final_input_for_reg_model = pd.DataFrame([all_input_features_combined], columns=reg_model_features_order)
# 
#     # --- CRITICAL FIX: Ensure all columns are float64 and handle any potential NaNs/Infs ---
#     for col in final_input_for_reg_model.columns: # Iterate through all columns in the DataFrame
#         # Convert to numeric, coercing any non-numeric value to NaN
#         final_input_for_reg_model[col] = pd.to_numeric(final_input_for_reg_model[col], errors='coerce')
#         # Fill any NaNs that might have been created by pd.to_numeric with 0.0
#         final_input_for_reg_model[col].fillna(0.0, inplace=True) # Fill NaNs with 0.0
# 
#     # After filling NaNs, ensure the Dtype is explicitly float64 for all columns (including dummies)
#     final_input_for_reg_model = final_input_for_reg_model.astype(np.float64)
# 
# 
#     # Check for NaNs or Infs explicitly *after* all cleaning
#     # This check is now on the fully cleaned DataFrame.
#     if final_input_for_reg_model.isnull().values.any() or \
#        np.isinf(final_input_for_reg_model).values.any():
#         st.error("Error: Input data still contains missing or infinite values after cleaning. This indicates a very persistent issue.")
#         st.write("Problematic data:\n", final_input_for_reg_model)
#         st.write("Check `final_input_for_reg_model.info()` for dtypes and non-null counts:")
#         st.write(final_input_for_reg_model.info())
#         st.stop()
# 
# 
#     # Scale numerical features using the LOADED regression scaler
#     # Get the data to scale. This will be a DataFrame subset.
#     data_to_scale = final_input_for_reg_model[numerical_features_for_model]
# 
#     # Convert to NumPy array for scaler. If it's a single column, .values will make it 1D.
#     # So we ensure it's 2D for the scaler.
#     data_for_scaler_transform = data_to_scale.values
# 
#     # Reshape if it's 1D (i.e., if numerical_features_for_model had only one feature)
#     if data_for_scaler_transform.ndim == 1:
#         data_for_scaler_transform = data_for_scaler_transform.reshape(-1, 1)
# 
#     final_input_for_reg_model[numerical_features_for_model] = scaler_reg.transform(data_for_scaler_transform)
# 
# 
#     # --- 5.2 Make Prediction with the Regression Model ---
#     predicted_thickness_loss_mm = reg_model.predict(final_input_for_reg_model)[0]
# 
#     # Ensure thickness loss doesn't exceed initial thickness (physical constraint)
#     if predicted_thickness_loss_mm < 0:
#         predicted_thickness_loss_mm = 0.0 # Cannot have negative loss
#     if predicted_thickness_loss_mm > initial_thickness:
#         predicted_thickness_loss_mm = float(initial_thickness) # Cannot lose more than 100% of thickness
# 
# 
#     # --- 5.3 Calculate Percentage Material Loss ---
#     predicted_material_loss_percent = (predicted_thickness_loss_mm / initial_thickness) * 100
#     if predicted_material_loss_percent > 100: # Cap at 100%
#         predicted_material_loss_percent = 100.0
# 
# 
#     # --- 5.4 Determine Condition based on Predicted Thickness Loss ---
#     predicted_condition = get_condition_from_thickness_loss(predicted_thickness_loss_mm)
# 
# 
#     # --- 6. Display Results (Wow Factor!) ---
#     st.subheader("üí° Predicted Degradation Metrics")
#     st.info(f"**Predicted Thickness Loss:** `{predicted_thickness_loss_mm:.2f} mm`")
#     st.info(f"**Calculated Percentage Material Loss:** `{predicted_material_loss_percent:.2f}%`")
#     st.caption("*(Note: Thickness Loss is predicted by a Machine Learning regression model. Percentage Material Loss is derived from it.)*")
# 
#     st.subheader("üéâ Predicted Pipeline Condition")
#     condition_style = ""
#     if predicted_condition == "Critical":
#         condition_style = "highlight"
#     elif predicted_condition == "Moderate":
#         condition_style = "moderate"
#     elif predicted_condition == "Normal":
#         condition_style = "normal"
# 
#     st.markdown(f"""
#         <div class="prediction-box">
#             <h3>Predicted Condition: <span class="{condition_style}">{predicted_condition}</span></h3>
#             <p>Based on the predicted degradation, the pipeline segment is classified as in a <span class="{condition_style}">{predicted_condition}</span> condition.</p>
#         </div>
#     """, unsafe_allow_html=True)
# 
#     st.markdown("---")
# 
#     # --- Wow Factor: Interpretation and Actionable Insights ---
#     st.subheader("Next Steps & Recommendations:")
#     if predicted_condition == "Critical":
#         st.error("üö® **Urgent Action Required!** This pipeline segment is in a CRITICAL condition. Immediate inspection and potential replacement or major repair are highly recommended to prevent failure and ensure safety.")
#         st.markdown("Consider scheduling emergency maintenance and re-routing if possible. Further investigation (e.g., in-line inspection, NDT) is crucial.")
#     elif predicted_condition == "Moderate":
#         st.warning("‚ö†Ô∏è **Attention Needed!** This pipeline segment is in a MODERATE condition. Regular monitoring should be intensified, and a detailed inspection (e.g., within the next 6-12 months) is advised to assess the degradation rate and plan for future maintenance.")
#         st.markdown("Prioritize this segment for upcoming routine inspections. Evaluate if operational adjustments can mitigate further degradation.")
#     elif predicted_condition == "Normal":
#         st.success("‚úÖ **Good Condition!** This pipeline segment is in a NORMAL condition. Continue with routine monitoring and scheduled maintenance as per your standard integrity management program.")
#         st.markdown("While currently stable, continuous monitoring and adherence to maintenance schedules are important to prevent future issues.")
# 
#     st.markdown("---")
#     st.markdown("### How this Prediction Works:")
#     st.info("""
#     This application uses a pre-trained **Machine Learning Regression Model** to predict 'Thickness Loss (mm)'.
#     'Percentage Material Loss (%)' is then calculated, and the final 'Condition' (Normal, Moderate, Critical) is determined based on industry-aligned thresholds of the predicted 'Thickness Loss'.
#     """)
#     st.markdown("---")
#     st.markdown("Developed for your Final Year Project. Enjoy!")